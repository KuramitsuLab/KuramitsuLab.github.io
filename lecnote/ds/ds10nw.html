

<!DOCTYPE html>
<html class="writer-html5" lang="ja" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>10. ニューラル・ネットワーク &mdash; Python &amp; Data Science 2021  ドキュメント</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="検索" href="search.html" />
    <link rel="next" title="11. 深層学習と画像認識" href="ds11deep.html" />
    <link rel="prev" title="9. データ・サイエンティストへの道" href="ds09ds.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Python & Data Science 2021
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="ds01intro.html">1. Python とデータサイエンス</a></li>
<li class="toctree-l1"><a class="reference internal" href="ds02sympy.html">2. 計算はPythonにさせてしまおう</a></li>
<li class="toctree-l1"><a class="reference internal" href="ds03numpy.html">3. NumPyとグラフ</a></li>
<li class="toctree-l1"><a class="reference internal" href="ds04pandas.html">4. 表データとPandasを使いこなそう</a></li>
<li class="toctree-l1"><a class="reference internal" href="ds05data.html">5. データの傾向をつかもう</a></li>
<li class="toctree-l1"><a class="reference internal" href="ds06classfy.html">6. クラスタリングしてみよう</a></li>
<li class="toctree-l1"><a class="reference internal" href="ds07reg.html">7. 回帰：線形近似から予測する</a></li>
<li class="toctree-l1"><a class="reference internal" href="ds08ml.html">8. 機械学習とクラス分類問題</a></li>
<li class="toctree-l1"><a class="reference internal" href="ds09ds.html">9. データ・サイエンティストへの道</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">10. ニューラル・ネットワーク</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#ニューラル・ネットワークの原理">10.1. ニューラル・ネットワークの原理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ニューロン">10.1.1. ニューロン</a></li>
<li class="toctree-l3"><a class="reference internal" href="#単純パーセプトロン">10.1.2. 単純パーセプトロン</a></li>
<li class="toctree-l3"><a class="reference internal" href="#確率モデルの導入">10.1.3. 確率モデルの導入</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ニューラルネットワークの学習">10.1.4. ニューラルネットワークの学習</a></li>
<li class="toctree-l3"><a class="reference internal" href="#勾配降下法">10.1.5. 勾配降下法</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#単純パーセプトロンの実装">10.2. 単純パーセプトロンの実装</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#活性化関数">10.2.1. 活性化関数</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id12">10.2.2. 勾配降下法</a></li>
<li class="toctree-l3"><a class="reference internal" href="#パーセプトロンの原理">10.2.3. パーセプトロンの原理</a></li>
<li class="toctree-l3"><a class="reference internal" href="#エポック(epoch)">10.2.4. エポック(epoch)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#多層パーセプトロン">10.3. 多層パーセプトロン</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#線形分離不可能な問題">10.3.1. 線形分離不可能な問題</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id16">10.3.2. 多層パーセプトロン</a></li>
<li class="toctree-l3"><a class="reference internal" href="#深層学習へ">10.3.3. 深層学習へ</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#コースワーク">10.4. コースワーク</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ds11deep.html">11. 深層学習と画像認識</a></li>
<li class="toctree-l1"><a class="reference internal" href="ds12nlp.html">12. 自然言語処理にむけて</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Python & Data Science 2021</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li><span class="section-number">10. </span>ニューラル・ネットワーク</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/ds10nw.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="ニューラル・ネットワーク">
<h1><span class="section-number">10. </span>ニューラル・ネットワーク<a class="headerlink" href="#ニューラル・ネットワーク" title="このヘッドラインへのパーマリンク">¶</a></h1>
<p>深層学習は、2010年代以降の人工知能ブームの立役者です。 深層学習を理解するため、ニューラルネットワークと基礎原理をしっかり理解しておきましょう。</p>
<div class="section" id="ニューラル・ネットワークの原理">
<h2><span class="section-number">10.1. </span>ニューラル・ネットワークの原理<a class="headerlink" href="#ニューラル・ネットワークの原理" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>ニューラル・ネットワーク(neural network)は、人間の脳の構造を模した人工知能アルゴリズムです。</p>
<div class="section" id="ニューロン">
<h3><span class="section-number">10.1.1. </span>ニューロン<a class="headerlink" href="#ニューロン" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>人間の脳は、ニューロン(neuron)と呼ばれる神経細胞から構成されます。</p>
<p>ニューロンを単純化した数理モデルで考えます。</p>
<img alt="neuron-fs8.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/57754/f4485c8d-5f40-e478-a95f-043071b17ba6.png" />
<p>ネットワークの<strong>重み</strong>を<span class="math notranslate nohighlight">\(w_1, w_2, ..., w_i, ..\)</span>とすると：</p>
<p><strong>ニューロンから伝わる信号の総量</strong></p>
<div class="math notranslate nohighlight">
\[w_1 x_1 + w_2 x_2 + ... + w_i x_i + ...\]</div>
<p>発火：次のニューロンに信号を伝える</p>
<p>入力の信号量がある閾値<span class="math notranslate nohighlight">\(\theta\)</span>を超えるかどうかで決まる</p>
<ul class="simple">
<li><p>発火　<span class="math notranslate nohighlight">\((w_1 x_1 + w_2 x_2 + ... + w_i x_i + ... \ge \theta)\)</span></p></li>
<li><p>発火しない <span class="math notranslate nohighlight">\((w_1 x_1 + w_2 x_2 + ... w_i x_i + ... &lt; \theta)\)</span></p></li>
</ul>
<p><strong>(学習のイメージ）誤り訂正学習法</strong></p>
<p>入力<span class="math notranslate nohighlight">\((x_1, x_2, ..., x_i, ...)\)</span>に対し、 ある重み<span class="math notranslate nohighlight">\((w_1, w_2, ..., w_i, ...)\)</span>で出力<span class="math notranslate nohighlight">\(y\)</span>を計算し、 出力が誤っていたら重みを調整することで、徐々に正しい出力を出すように近づけます。</p>
</div>
<div class="section" id="単純パーセプトロン">
<h3><span class="section-number">10.1.2. </span>単純パーセプトロン<a class="headerlink" href="#単純パーセプトロン" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>単純パーセプトロンは、ニューラルネットワークの単純な数理モデルです。</p>
<p>$ y = f(<span class="math">\mathbf{w}</span><span class="math">\cdot</span>:nbsphinx-math:<a href="#id5"><span class="problematic" id="id6">`</span></a>mathbf{x}`+b) $</p>
<ul class="simple">
<li><p>入力: <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, x_2, ..., x_n)\)</span></p></li>
<li><p>重み: <span class="math notranslate nohighlight">\(\mathbf{w} = (w_1, w_2, ..., w_n)\)</span></p></li>
<li><p>バイアス: <span class="math notranslate nohighlight">\(b\)</span></p></li>
<li><p>活性化関数: <span class="math notranslate nohighlight">\(f\)</span></p></li>
</ul>
<p>活性化関数は、ニューロンの発火を定める関数です。 前の説明では、ニューロンの発火を0,1で決定していました。 それを関数として表すと、次のような<strong>ステップ関数</strong>となります。</p>
<p><strong>ステップ関数</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}f(x) = \begin{cases}
    1 &amp; (x&gt;0) \\
    0  &amp; (x\le0)
  \end{cases}\end{split}\]</div>
<p>しかし、 ステップ関数は、発火しそうだけどギリギリ発火しないなどの中間的な状態が表現できません。 単純関数パーセプトロンでは、ステップ関数の代わりに、 ロジスティック回帰でも用いた<strong>標準シグモイド関数</strong>を使うことで、 0から1の連続値を扱えるようになります。 これで、0.49のようなギリギリ発火しない状態も表現できます。</p>
<p><strong>標準シグモイド関数</strong></p>
<div class="math notranslate nohighlight">
\[\sigma(x) = \frac{1}{1+e^{-x}}\]</div>
<p>ちなみに、シグモイド関数が好まれる理由は、微分をしてみると、シグモイド関数の形で表されるからです。</p>
<div class="math notranslate nohighlight">
\[\sigma'(x) = \sigma(x)(1 - \sigma(x))\]</div>
</div>
<div class="section" id="確率モデルの導入">
<h3><span class="section-number">10.1.3. </span>確率モデルの導入<a class="headerlink" href="#確率モデルの導入" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>シグモイド関数を用いると、確率的分類モデルになります。</p>
<ul class="simple">
<li><p><strong>発火する確率</strong>: <span class="math notranslate nohighlight">\(p(C = 1 ~|~ \mathbf{x}) = \sigma(\mathbf{w} \mathbf{x} + b)\)</span></p></li>
<li><p><strong>発火しない確率</strong>: <span class="math notranslate nohighlight">\(p(C = 0~|~ \mathbf{x}) = 1 - p(C = 1 ~|~ \mathbf{x}) = 1 - \sigma(\mathbf{w} \mathbf{x} + b)\)</span></p></li>
</ul>
<p>確率変数<span class="math notranslate nohighlight">\(C\)</span>は、0か1なので、<span class="math notranslate nohighlight">\(y = \mathbf{w} \mathbf{x} + b\)</span>として、 次の式にまとめられます。</p>
<div class="math notranslate nohighlight">
\[p(C = t | \mathbf{x}) = y^t(1-y)^{(1-t)}\]</div>
<p>尤度関数(ゆうどかんすう）は、ある前提条件に従って結果が出現する場合に、逆に観察結果からみて前提条件が「何々であった」と推測する尤もらしさ（もっともらしさ）を表す数値を関数として捉えたものです。</p>
<p>尤度関数: <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>と<span class="math notranslate nohighlight">\(b\)</span>を尤度推定するための関数</p>
<div class="math notranslate nohighlight">
\[L(\mathbf{w}, b) = \prod_{n=1}^{N} p(C = t_n|\mathbf{x}_n) = \prod_{n=1}^{N} y_n^{t_n}(1 - y_n)^{1-t_n}\]</div>
<p>なお、突然出てきた<span class="math notranslate nohighlight">\(n=1,...,N\)</span>はデータの件数です。</p>
</div>
<div class="section" id="ニューラルネットワークの学習">
<h3><span class="section-number">10.1.4. </span>ニューラルネットワークの学習<a class="headerlink" href="#ニューラルネットワークの学習" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>ニューラル・ネットワークの学習は、 尤度関数<span class="math notranslate nohighlight">\(L(w, b)\)</span>を最大化するように<span class="math notranslate nohighlight">\(w\)</span>と<span class="math notranslate nohighlight">\(b\)</span>を調整することになります。</p>
<div class="admonition note">
<p class="admonition-title fa fa-exclamation-circle"><strong>最適化問題(optimization problem)</strong></p>
<p>関数が最大・最小となる状態を求める問題のこと。 関数の最大化は、符号を反転すると、最小化に置き換えられるので、 一般に関数を最適化するとは、関数を最小化するパラメータを求めることです。</p>
</div>
<p>最適化問題となれば、パラメータの偏微分（勾配）を求め、勾配が<span class="math notranslate nohighlight">\(0\)</span>になるパラメータを探します。 ただし、積の形をしているので、偏微分の計算が煩雑になります。そこで、事前の準備として、対数をとって、和の形に変形しておきます。</p>
<p><strong>交差エントロピー誤差関数(cross-entropy error function)</strong></p>
<div class="math notranslate nohighlight">
\[E(\mathbf{w}, b) = - \log{L(\mathbf{w}, b)} = - \sum_{n=1}^{N} t_n \log{y_n} + (1 - t_n)\log{1-y_n}\]</div>
<p><span class="math notranslate nohighlight">\(E(\mathbf{w}, b)\)</span>を最小化することがもともとの尤度関数の最適化になります。一般的には、<span class="math notranslate nohighlight">\(E\)</span>のことを<strong>誤差関数(error function)</strong>、もしくは、<strong>損失関数(loss function)</strong>と呼びます。</p>
</div>
<div class="section" id="勾配降下法">
<h3><span class="section-number">10.1.5. </span>勾配降下法<a class="headerlink" href="#勾配降下法" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>交差エントロピー誤差関数<span class="math notranslate nohighlight">\(E(\mathbf{w},b)\)</span>を最適化するためには、<span class="math notranslate nohighlight">\(\mathbf{w}, b\)</span>で偏微分して0になるパラメータを求めることになります。しかし、解析的にこの値を求めるのは困難な場合があります。 そこで、パラメータを逐次的に更新することで、最適化を探索するアプローチがとられます。</p>
<p><strong>勾配降下法(gradient descent)</strong></p>
<div class="math notranslate nohighlight">
\[\mathbf{w}^{(k+1)} = \mathbf{w}^{(k)} - \eta \frac{\partial E(\mathbf{w}, b)}{\partial w}
= \mathbf{w}^{(k)} - \eta \sum_{n=1}^{N}(t_n - y_n)\mathbf{x}_n\]</div>
<div class="math notranslate nohighlight">
\[b^{(k+1)} = b^{(k)} - \eta \frac{\partial E(\mathbf{w}, b)}{\partial b}
= b^{(k)} - \eta \sum_{n=1}^N (y_n - t_n)\]</div>
<p>(直感的な解釈)： 予測値と実際の値との誤差(y_n - t_n)を用いて、パラメータが更新されます。つまり、ニューラルネットワークの目標は、「予測値と実際の値」の差をなくすことなので、直感に反しない解釈となります。</p>
<div class="admonition note">
<p class="admonition-title fa fa-exclamation-circle">学習率(learning_rate): <span class="math notranslate nohighlight">\(\eta(&gt;0)\)</span></p>
<p>学習率は、収束しやすさを調整するハイパーパラメータです。 通常は、<span class="math notranslate nohighlight">\(0.1\)</span>や<span class="math notranslate nohighlight">\(0.01\)</span>などの適当な小さい値を与えます。</p>
</div>
</div>
</div>
<div class="section" id="単純パーセプトロンの実装">
<h2><span class="section-number">10.2. </span>単純パーセプトロンの実装<a class="headerlink" href="#単純パーセプトロンの実装" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>NumPyを使って、単純パーセプトロンを実装してみましょう。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

</pre></div>
</div>
</div>
<div class="section" id="活性化関数">
<h3><span class="section-number">10.2.1. </span>活性化関数<a class="headerlink" href="#活性化関数" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>まず、活性化関数をステップ関数とシグモイド関数を比較してみましょう。</p>
<p><strong>ステップ関数</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}f(x) = \begin{cases}
    1 &amp; (x&gt;0) \\
    0  &amp; (x\le0)
  \end{cases}\end{split}\]</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>

<span class="n">step</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frompyfunc</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># ユニバーサル関数へ変換</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[&lt;matplotlib.lines.Line2D at 0x11cf03190&gt;]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/ds10nw_7_1.svg" src="_images/ds10nw_7_1.svg" /></div>
</div>
<p><strong>標準シグモイド</strong></p>
<div class="math notranslate nohighlight">
\[\sigma(x) = \frac{1}{1+e^{-x}}\]</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[&lt;matplotlib.lines.Line2D at 0x11ce44d60&gt;]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/ds10nw_9_1.svg" src="_images/ds10nw_9_1.svg" /></div>
</div>
</div>
<div class="section" id="id12">
<h3><span class="section-number">10.2.2. </span>勾配降下法<a class="headerlink" href="#id12" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p><strong>勾配降下法(GD)</strong>は、関数<span class="math notranslate nohighlight">\(f(x)\)</span>があったとき、<span class="math notranslate nohighlight">\(f(x)\)</span>を最小にするような<span class="math notranslate nohighlight">\(x\)</span>(<span class="math notranslate nohighlight">\({\mathop{\rm arg~max}\limits}_{x}　f(x)\)</span>とも書く)を求める手法です。 最急降下法（Gradient descent, steepest descent）と呼ばれることもあります。</p>
<p><strong>原理</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}x_{i+1} = x_i - \eta f'(x_i)\\
(x_{i+1}, y_{i+1}) = (x_i, y_i) - \eta (\frac{\partial f(x_i, y_i)}{\partial x_i}, \frac{\partial f(x_i, y_i)}{\partial y_i})\end{split}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f'(x_i), \frac{\partial f(x_i, y_i)}{\partial x_i}, \frac{\partial f(x_i, y_i)}{\partial y_i}\)</span>: 勾配</p></li>
<li><p><span class="math notranslate nohighlight">\(\eta\)</span>: 学習率（ハイパーパラメータ）</p></li>
</ul>
<p><strong>1変数の場合</strong></p>
<p>例として、次の簡単な放物線を考えることにします。</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(x) = 3x^4 −4x^3 −12x^2 +3\\
f'(x) = 12x^3 − 12x^2 − 24x \\\end{split}\]</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[42]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">12</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">24</span>

<span class="k">def</span> <span class="nf">df</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">12</span><span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">12</span><span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">24</span><span class="o">*</span><span class="n">x</span>

<span class="k">def</span> <span class="nf">show_grad2</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">show_grad2</span><span class="p">()</span>

</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/ds10nw_11_0.svg" src="_images/ds10nw_11_0.svg" /></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[43]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">eta</span> <span class="o">=</span> <span class="mf">0.001</span>   <span class="c1"># 学習率</span>

<span class="n">x</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># 初期値</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">df</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>  <span class="c1"># 勾配降下法</span>

<span class="n">show_grad2</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[43]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.collections.PathCollection at 0x11f3bf040&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/ds10nw_12_1.svg" src="_images/ds10nw_12_1.svg" /></div>
</div>
<div class="admonition note">
<p class="admonition-title fa fa-exclamation-circle">大域最適性と局所最適性</p>
<p>局所最適性とは、最適解のひとつであるが、必ずしも最小にならないこと。 （最小となる最適解を大域最適解と呼ぶ。）</p>
</div>
<p>勾配降下法は、大域最小性は保証されません。例えば、上の例でも初期値を<span class="math notranslate nohighlight">\(x=-3\)</span>で始めると、局所最適解に収束してしまいます。学習率などを変更して、山を超えるように調整することもできます。 ただし、機械学習では調整するパラメータの数が多いので、多少、局所最適解になるパラメータがあっても、計算の効率が優先されます。</p>
<p><strong>2変数の場合</strong></p>
<div class="math notranslate nohighlight">
\[z = x^2 + y^2 ~~~
\frac{\partial z}{\partial x} = 2x ~~\frac{\partial z}{\partial y} = 2y\]</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">y</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">z</span>

<span class="k">def</span> <span class="nf">show_grads</span><span class="p">():</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">Axes3D</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>

    <span class="n">ax1</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;f(x, y)&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ax1</span>

<span class="n">show_grads</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/ds10nw_14_0.svg" src="_images/ds10nw_14_0.svg" /></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10</span>  <span class="c1"># 初期値</span>
<span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># 初期値</span>

<span class="k">def</span> <span class="nf">dfdx</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">dfdy</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y</span>


<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dfdx</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>  <span class="c1"># 勾配降下法</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dfdy</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>  <span class="c1"># 勾配降下法</span>

<span class="n">ax1</span> <span class="o">=</span> <span class="n">show_grads</span><span class="p">()</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">scatter3D</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/ds10nw_15_0.svg" src="_images/ds10nw_15_0.svg" /></div>
</div>
<p>最先端のニューラルネットワークでは、勾配降下法に完成項(momentum)を追加させた手法、さらに適応的に学習率を変更するAdaGradが使われています。</p>
<ul class="simple">
<li><p>勾配降下法(GD): <span class="math notranslate nohighlight">\(\mathbf{x}_{i+1} = \mathbf{x}_i - \eta \nabla f\)</span></p></li>
<li><p>Momentum: <span class="math notranslate nohighlight">\(\mathbf{x}_{i+1} = \mathbf{x}_i - \eta \nabla f+ \alpha \Delta \mathbf{w}\)</span></p></li>
<li><p>AdaGrad: <span class="math notranslate nohighlight">\(\mathbf{x}_{i+1} = \mathbf{x}_i - \eta \frac{1}{\sqrt{h_i}}\nabla f\)</span>, <span class="math notranslate nohighlight">\(h_{i+1} = h_i + (\nabla f)^2\)</span></p></li>
</ul>
<p><img alt="8eb7eaed05024abd94a15b3623f0d098" class="no-scaled-link" src="https://watlab-blog.com/wp-content/uploads/2020/03/gd-momentum-adagrad.gif" style="width: 50%;" /></p>
<p><a class="reference external" href="https://watlab-blog.com/2020/03/08/adagrad/">wat氏のわかりやすい解説</a>より引用</p>
</div>
<div class="section" id="パーセプトロンの原理">
<h3><span class="section-number">10.2.3. </span>パーセプトロンの原理<a class="headerlink" href="#パーセプトロンの原理" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>単純パーセプトロンの動作原理を確認していましょう。</p>
<p><strong>サンプルデータ</strong></p>
<table class="docutils align-default">
<colgroup>
<col style="width: 48%" />
<col style="width: 48%" />
<col style="width: 4%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(x_1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(x_2\)</span></p></th>
<th class="head"><p>t</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[44]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x.shape&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;t.shape&#39;</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x.shape (4, 2)
t.shape (4,)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[45]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">DIM</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1">#2</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">DIM</span><span class="p">,))</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;w =&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;b =&#39;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
w = [ 2.84961805 -1.06240844]
b = 0
</pre></div></div>
</div>
<p>次は、ニューロンのモデルを計算してみましょう。</p>
<div class="math notranslate nohighlight">
\[y = \mathbf{w} \mathbf{x}^T + b\]</div>
<p>今回の入力は、４回分の入力が<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>に入っています。 これをバッチ処理として、一回で計算してしまいます。</p>
<div class="admonition note">
<p class="admonition-title fa fa-exclamation-circle">バッチ処理</p>
<p>複数の入力データをまとめて計算すること。 （GPUの性能を引き出すときに必須のテクニックです。）</p>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[55]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;y = &#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
y =  [ 0.         -1.06240844  2.84961805  1.78720961]
</pre></div></div>
</div>
<p>活性化関数を適用して、[0.0, 1.0]の範囲にします。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[59]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">+</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;y = &#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
y =  [0.5        0.25684947 0.94529894 0.85658483]
</pre></div></div>
</div>
<p><span class="math notranslate nohighlight">\(y\)</span>は、ニューロンから予想された出力になります。ここから正解<span class="math notranslate nohighlight">\(t\)</span>との差分を計算します。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[50]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">delta</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">t</span>
<span class="n">delta</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[50]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([ 0., -1., -2., -3.])
</pre></div></div>
</div>
<p>勾配効果法で1ステップだけ進めてみましょう。</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{w}^{(k+1)} = = \mathbf{w}^{(k)} - \eta \sum_{n=1}^{N}(t_n - y_n)\mathbf{x}_n\\
b^{(k+1)} == b^{(k)} - \eta \sum_{n=1}^N (y_n - t_n)\end{split}\]</div>
<p>積の和を<code class="docutils literal notranslate"><span class="pre">np.matmul()</span></code>を使って一度に計算しています。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[61]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># 学習率</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;BEFORE&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;w =&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;b =&#39;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;delta =&#39;</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
<span class="n">dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
<span class="n">db</span> <span class="o">=</span> <span class="n">delta</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dw</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">db</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;w =&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;b =&#39;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
BEFORE
w = [ 2.84961805 -1.06240844]
b = 0
delta = [ 0. -1. -2. -3.]
w = [ 3.34961805 -0.66240844]
b = 0.6000000000000001
</pre></div></div>
</div>
<p>損失関数は、交差エントロピー誤差から計算します。</p>
<div class="math notranslate nohighlight">
\[E(\mathbf{w}, b) = - \sum_{n=1}^{N} t_n \log{y_n} + (1 - t_n)\log{1-y_n}\]</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[60]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">compute_loss</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[60]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-4.160569009889807
</pre></div></div>
</div>
</div>
<div class="section" id="エポック(epoch)">
<h3><span class="section-number">10.2.4. </span>エポック(epoch)<a class="headerlink" href="#エポック(epoch)" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>何回か繰り返し計算することで収束します。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[77]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#乱数を固定</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span> <span class="c1"># 初期の重み</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">epoch</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">yloss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">t</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">delta</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;y[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">]=&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;loss[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
    <span class="n">yloss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="c1"># グラフ描画用</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;正解: &#39;</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;w =&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="s1">&#39;b =&#39;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">),</span> <span class="n">yloss</span><span class="p">)</span>

</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
y[1]= [0.50752788 0.62974618 0.86342404 0.91253912] loss[1] 1.4091295317036505
y[6]= [0.51019507 0.71891883 0.88585086 0.95013865] loss[6] 1.2161089046643379
y[11]= [0.48862562 0.76099054 0.89251596 0.96511954] loss[11] 1.0930018548716502
y[16]= [0.46066769 0.78669183 0.89492895 0.97352857] loss[16] 0.995181106687431
y[21]= [0.4322263  0.805267   0.89619061 0.97912107] loss[21] 0.9133158613216121
y[26]= [0.40540246 0.82010343 0.89732886 0.98317538] loss[26] 0.8434959439212844
y[31]= [0.38084255 0.83262494 0.89867666 0.98625075] loss[31] 0.7832442968588098
y[36]= [0.3586118  0.84350989 0.90030565 0.98864412] loss[36] 0.7307458503091289
y[41]= [0.33855443 0.85312782 0.90219015 0.9905373 ] loss[41] 0.6846112082276821
y[46]= [0.32044591 0.86170934 0.90427376 0.99205254] loss[46] 0.6437580612391854
y[51]= [0.30405586 0.86941615 0.9064965  0.99327685] loss[51] 0.6073332350454044
y[56]= [0.28917204 0.87637157 0.90880503 0.9942743 ] loss[56] 0.5746568096002935
y[61]= [0.27560752 0.88267474 0.91115551 0.99509307] loss[61] 0.5451811355066014
y[66]= [0.26320093 0.88840785 0.91351359 0.99576989] loss[66] 0.5184605273190003
y[71]= [0.25181408 0.8936402  0.91585317 0.99633308] loss[71] 0.49412869982463403
y[76]= [0.24132881 0.89843086 0.91815503 0.99680465] loss[76] 0.4718818362396215
y[81]= [0.23164401 0.90283047 0.92040537 0.99720186] loss[81] 0.45146576789207765
y[86]= [0.22267285 0.90688269 0.92259466 0.99753832] loss[86] 0.4326661729160132
y[91]= [0.21434046 0.91062524 0.92471665 0.99782485] loss[91] 0.4153010085768411
y[96]= [0.20658203 0.91409088 0.9267676  0.9980701 ] loss[96] 0.3992146105506623
正解:  [0 1 1 1]
w = [3.94331525 3.77972392] b = -1.381611021055158
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[77]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[&lt;matplotlib.lines.Line2D at 0x11e55c1c0&gt;]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/ds10nw_31_2.svg" src="_images/ds10nw_31_2.svg" /></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="多層パーセプトロン">
<h2><span class="section-number">10.3. </span>多層パーセプトロン<a class="headerlink" href="#多層パーセプトロン" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>単純パーセプトロンは、原理的にロジスティック回帰と等価です。 （実装で頑張っても）<strong>線形分離可能な問題</strong>しか解くことができません。</p>
<div class="admonition note">
<p class="admonition-title fa fa-exclamation-circle"><strong>線形分離可能な問題</strong></p>
<p>幾何学においてふたつの集合が二次元平面上にあるとき、それらの集合を一本の直線で分離できることです。</p>
<p><strong>線形分離可能な例</strong></p>
<p><img alt="c2ee10eafbf74e6a8156a52faeb45ed5" class="no-scaled-link" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/57754/1dd6e067-1875-27a8-fbf4-e81158f54719.png" style="width: 40%;" /></p>
<p>一般化して、<span class="math notranslate nohighlight">\(n\)</span> 次元空間上のふたつの集合を <span class="math notranslate nohighlight">\(n − 1\)</span> 次元の超平面で 分離できることも線形分離可能と呼びます。 逆に、線形分離不可能な問題を線形分離不可能問題と呼びます。</p>
</div>
<div class="section" id="線形分離不可能な問題">
<h3><span class="section-number">10.3.1. </span>線形分離不可能な問題<a class="headerlink" href="#線形分離不可能な問題" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>線形分離不可能な問題は、日常的に存在します。例えば、論理回路のXOR回路も設計分離不可能です。</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 48%" />
<col style="width: 48%" />
<col style="width: 4%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(x_1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(x_2\)</span></p></th>
<th class="head"><p>t</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p><strong>線形分離不可能な例</strong></p>
<p>赤い部分が分離できていない。</p>
<p><img alt="ff19af70179740178a369ad01d7da398" class="no-scaled-link" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/57754/db9d8fa2-05e1-6dd5-e3ef-09582a1b0b1b.png" style="width: 40%;" /></p>
<div class="admonition note">
<p class="admonition-title fa fa-exclamation-circle">Let’s try</p>
<p>単純パーセプトロンの実装コードを使って、線形不分離問題を解こうとするとどうなるか調べてみよう。</p>
</div>
</div>
<div class="section" id="id16">
<h3><span class="section-number">10.3.2. </span>多層パーセプトロン<a class="headerlink" href="#id16" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>線形分離不可能な問題を解くためのアイディアは、パーセプトロンを組み合わせて多層化することです。</p>
<p><strong>３層パーセプトロンの例</strong></p>
<p><img alt="34bc28fef184421ba35e4b9ea735fc82" class="no-scaled-link" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/57754/1c2f261f-701d-e020-2f49-c3df596b8363.png" style="width: 60%;" /></p>
<p>このように多層化することで、各ニューロンの学習結果が組み合わさって、 線形分離不可能な問題でも複数の分離線の組み合わせで分離可能になります。</p>
<p><img alt="515ce0b54d0a4e6386197d4f30b1545a" class="no-scaled-link" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/57754/5ce7c474-466c-7889-f3d4-19277c815289.png" style="width: 40%;" /></p>
<p>さて、「多層パーセプトロンも実装してみましょう！」としたいところですが、 ちょっと１回の講義では多すぎる気がしますので重要な概念だけ紹介しておきます。</p>
<div class="admonition note">
<p class="admonition-title fa fa-exclamation-circle">逆誤差伝搬法</p>
<p>ニューロンは、順方向に発火させていきますが、学習を効率よく行うため、誤差を逆に伝搬させます。</p>
<p><img alt="72afc371f68d4aa098417279ce8ec55f" class="no-scaled-link" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/57754/49a140bb-5aa0-45a7-a365-7c3aa5c914b6.png" style="width: 60%;" /></p>
</div>
</div>
<div class="section" id="深層学習へ">
<h3><span class="section-number">10.3.3. </span>深層学習へ<a class="headerlink" href="#深層学習へ" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>多層パーセプトロンは、４層以上の局所最適解や勾配消失などの技術的な問題によって、十分に学習させられず、性能も芳しくなく、1990年代は機械学習のメインストリームから外れていました。</p>
<p>深層学習は、2006年に、ジェフリー・ヒントンらがオートエンコーダを提案し、多層でも十分に学習できるように改善したニューラルネットワークです。</p>
<div class="admonition note">
<p class="admonition-title fa fa-exclamation-circle">オートエンコーダ</p>
<p>ニューラルネットワークを使用した次元圧縮のためのアルゴリズム</p>
</div>
<p><strong>Nature誌に掲載されたDeep Learning の発明者らの論文</strong></p>
<p><a class="reference external" href="https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf">https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf</a></p>
<img alt="deep1-fs8.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/57754/ebc23dd0-544b-7b72-185e-510a584aae8b.png" />
<img alt="deep2-fs8.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/57754/49663032-edde-3f61-c00b-cc1dcf5d0704.png" />
</div>
</div>
<div class="section" id="コースワーク">
<h2><span class="section-number">10.4. </span>コースワーク<a class="headerlink" href="#コースワーク" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>今回は、少々難解な数式が出て、コードも「とっ散らかった」感じになってしまいました。 このようなコードは、ライブラリとしてまとめておくと、再利用しやすくなります。</p>
<div class="admonition tip"><p><strong>演習（SinglePerceptron クラス）</strong></p>
<p>Python 言語のクラスを定義を使って、 単純パーセプトロンのコードを sklearn 風に学習と予測ができるようにまとめてみましょう。</p>
<p>モデル生成：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SinglePerceptron</span><span class="p">(</span><span class="n">lerning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
<p>学習：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>予測：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div></div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="ds11deep.html" class="btn btn-neutral float-right" title="11. 深層学習と画像認識" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="ds09ds.html" class="btn btn-neutral float-left" title="9. データ・サイエンティストへの道" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, 倉光君郎(Kimio Kuramitsu).

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>